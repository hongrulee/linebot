{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "899b496e-72db-4c80-a610-6df0ff3b64f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\W20240908\\Lib\\site-packages\\whisper\\__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba440f750eb2455285d68b4bc5bf19a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad507f55cb44aa28f86a6fc5bf03615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0989feb306b346cbad5d06ec1722f88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4d5c560f1346579a715da1c6158ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a56b1e7bffb40b48a79741930e438df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e9651aaf3b4708beb9d68220be2bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5aba45056942f0ad5c94b21d0faf15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:10000\n",
      " * Running on http://192.168.0.199:10000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Sep/2024 21:40:01] \"POST /qa HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from flask import Flask, request, abort, render_template, redirect, url_for, flash, jsonify, send_file\n",
    "from linebot import LineBotApi, WebhookHandler\n",
    "from linebot.exceptions import InvalidSignatureError\n",
    "from linebot.models import (\n",
    "    MessageEvent, AudioMessage, TextMessage, TextSendMessage, AudioSendMessage\n",
    ")\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "#---------------------------語音辨識模型載入\n",
    "import tempfile\n",
    "import whisper\n",
    "# 載入 Whisper 模型\n",
    "model = whisper.load_model(\"tiny\")\n",
    "\n",
    "#---------------------------語言模型載入\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import opencc\n",
    "# 載入模型和分詞器\n",
    "model_name = \"Qwen/Qwen2-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# 檢查是否有可用的 GPU，如果有則將模型載入到 GPU 上，否則使用 CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name, ignore_mismatched_sizes=True).to(device)\n",
    "\n",
    "# 建立一個 OpenCC 轉換器，用於將簡體中文轉換為繁體中文\n",
    "converter = opencc.OpenCC('s2t')  # 's2t' 表示從簡體轉為繁體\n",
    "def answer_question(question):\n",
    "    # 將輸入的問題進行編碼，並生成 attention_mask\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", padding=True)   \n",
    "    # 將編碼後的輸入向量傳到到與模型相同的 device 上（GPU 或 CPU）\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}   \n",
    "    # 設定 pad_token_id 為 eos_token_id，以避免在處理填充時出現混淆\n",
    "    pad_token_id = tokenizer.eos_token_id   \n",
    "    # 使用 LLM 模型生成答案，這裡設置 max_new_tokens 來限制生成的最大長度\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'], \n",
    "        attention_mask=inputs['attention_mask'], \n",
    "        max_new_tokens=256, \n",
    "        pad_token_id=pad_token_id\n",
    "    )   \n",
    "    # 解碼生成的答案，將模型輸出的 token 轉換為人類可讀的內容\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)   \n",
    "    # 將簡體中文答案轉換為繁體中文\n",
    "    answer_traditional = converter.convert(answer)   \n",
    "    return answer_traditional\n",
    "\n",
    "\n",
    "\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.secret_key = 'os.urandom(24)'  # 用於未確定 key 前，先預設一組 key\n",
    "\n",
    "# 設置Log記錄\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# 設定 LINE CHANNEL ACCESS TOKEN 和 CHANNEL SECRET\n",
    "LINE_CHANNEL_ACCESS_TOKEN = 'w4627SjiixmfjJ7LNg6U8q9L8Nh+NXgaN4ELtQ9FkxjO8oO0aVdT8L9J9eGT/qNM9IrLMzjcngjmCtPy+Qa70dxtU0e4e8F6NA6hwbIM3lppgmzwNMiC257n6Eq8eLt+buQ8lSfFFNQF1AJvRZGRIgdB04t89/1O/w1cDnyilFU='\n",
    "LINE_CHANNEL_SECRET = '1d982942ffefc23710b07c6abc050cb1'\n",
    "SERVER_URL = 'your_server_url'  # 設定你的 SERVER_URL\n",
    "\n",
    "STT_API_URL = 'http://180.218.16.187:30303/recognition_long_audio'\n",
    "TTS_API_URL = 'http://180.218.16.187:30303/getTTSfromText'\n",
    "LLM_API_URL = 'http://61.66.218.215:30315/llm_chat'\n",
    "SERVER_PORT = 10000  # 免費空間 Render.com 預設 PORT\n",
    "\n",
    "line_bot_api = LineBotApi(LINE_CHANNEL_ACCESS_TOKEN)\n",
    "line_handler = WebhookHandler(LINE_CHANNEL_SECRET)\n",
    "\n",
    "# 語音轉文字 (需先架設好 Whisper Server)\n",
    "def get_text_from_audio(audio_path):\n",
    "    payload = {'doStyle': '0'}\n",
    "    files = [\n",
    "        ('audio', (os.path.basename(audio_path), open(audio_path, 'rb'), 'audio/mpeg'))\n",
    "    ]\n",
    "    headers = {}\n",
    "    response = requests.request('POST', STT_API_URL, headers=headers, data=payload, files=files)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            logging.info(f\"STT=> {data}\")\n",
    "            return data.get('result', '無法辨識音訊')\n",
    "        except requests.exceptions.JSONDecodeError:\n",
    "            logging.error(\"Failed to decode JSON response\")\n",
    "            return '無法辨識音訊'\n",
    "    else:\n",
    "        logging.error(f\"STT API request failed with status code {response.status_code}\")\n",
    "        return '錄音語音品質不佳，請再試試。'\n",
    "\n",
    "# LLM語言模型 (需先架設好 LLM Server)\n",
    "def get_response_from_llm(query):\n",
    "    payload = {'token': 'TEST',\n",
    "        'query': query,\n",
    "        'prompt_name': '艾妮機器人',\n",
    "        'max_tokens': '1024'}\n",
    "    files = []\n",
    "    headers = {}\n",
    "    response = requests.post(LLM_API_URL, headers=headers, data=payload, files=files)\n",
    "    data = response.json()\n",
    "    logging.info(f\"LLM=> {data}\")\n",
    "    return data.get('result', '無法獲取回應')\n",
    "\n",
    "# 文字轉語音  (需先架設好 TTS Server)\n",
    "def get_audio_from_text(text):\n",
    "    payload = {\n",
    "        'tone': '0',  # 語音音高\n",
    "        'speed': '0',  # 語音速度\n",
    "        'content': text,  # 語音內容\n",
    "        'gender': '1'  # 語音性別\n",
    "    }\n",
    "    headers = {}\n",
    "    response = requests.post(TTS_API_URL, headers=headers, data=payload)\n",
    "    audio_path = f'static/{int(time.time())}.mp3'\n",
    "    with open(audio_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    return audio_path\n",
    "\n",
    "def runcmd(command):\n",
    "    try:\n",
    "        # 使用 subprocess 執行命令行指令\n",
    "        ret = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding=\"utf-8\", timeout=3600)\n",
    "        if ret.returncode == 0:  # 檢查是否成功執行\n",
    "            print(\"成功:\", ret.stdout)\n",
    "        else:\n",
    "            print(\"錯誤:\", ret.stderr)\n",
    "            return False, ret.stderr  # 回傳錯誤訊息\n",
    "        return True, ret.stdout  # 回傳成功訊息\n",
    "    except subprocess.TimeoutExpired as e:\n",
    "        print(f\"命令超時: {e}\")\n",
    "        return False, f\"命令超時: {e}\"  # 回傳超時錯誤\n",
    "    except Exception as e:\n",
    "        print(f\"發生錯誤: {e}\")\n",
    "        return False, f\"發生錯誤: {e}\"  # 回傳一般錯誤\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def home():\n",
    "    return \"Line Bot 已啟動並運作\"\n",
    "\n",
    "def add_line_handlers(handler):\n",
    "    @handler.add(MessageEvent, message=AudioMessage)\n",
    "    def handle_audio_message(event):\n",
    "        message_content = line_bot_api.get_message_content(event.message.id)\n",
    "        audio_path = f'static/{int(time.time())}.mp3'\n",
    "\n",
    "        with open(audio_path, 'wb') as fd:\n",
    "            for chunk in message_content.iter_content():\n",
    "                fd.write(chunk)\n",
    "\n",
    "        text = get_text_from_audio(audio_path)\n",
    "        logging.info(f\"STT: {text}\")\n",
    "\n",
    "        llm_response = get_response_from_llm(text)[0]['content']\n",
    "        logging.info(f\"LLM Reply: {llm_response}\")\n",
    "\n",
    "        reply_audio_path = get_audio_from_text(llm_response)\n",
    "        logging.info(f\"TTS: {reply_audio_path}\")\n",
    "\n",
    "        if os.path.exists(reply_audio_path):\n",
    "            line_bot_api.reply_message(\n",
    "                event.reply_token,\n",
    "                [\n",
    "                    TextSendMessage(text=llm_response),\n",
    "                    AudioSendMessage(\n",
    "                        original_content_url=f'{SERVER_URL}/{reply_audio_path}',\n",
    "                        duration=330\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            line_bot_api.reply_message(\n",
    "                event.reply_token,\n",
    "                TextSendMessage(text=\"合成語音時錯誤，請檢查 TTS Server\")\n",
    "            )\n",
    "\n",
    "    @handler.add(MessageEvent, message=TextMessage)\n",
    "    def handle_text_message(event):\n",
    "        text = event.message.text\n",
    "        llm_response = get_response_from_llm(text)[0]['content']\n",
    "        logging.info(f\"LLM Reply: {llm_response}\")\n",
    "\n",
    "        reply_audio_path = get_audio_from_text(llm_response)\n",
    "        logging.info(f\"TTS: {reply_audio_path}\")\n",
    "\n",
    "        if os.path.exists(reply_audio_path):\n",
    "            line_bot_api.reply_message(\n",
    "                event.reply_token,\n",
    "                [\n",
    "                    TextSendMessage(text=llm_response),\n",
    "                    AudioSendMessage(\n",
    "                        original_content_url=f'{SERVER_URL}/{reply_audio_path}',\n",
    "                        duration=330\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            line_bot_api.reply_message(\n",
    "                event.reply_token,\n",
    "                TextSendMessage(text=\"合成語音時錯誤，請檢查 TTS Server\")\n",
    "            )\n",
    "\n",
    "@app.route(\"/webhook\", methods=[\"POST\"])\n",
    "def callback():\n",
    "    if not line_handler:\n",
    "        abort(500, \"LINE bot has not been configured.\")\n",
    "\n",
    "    signature = request.headers[\"X-Line-Signature\"]\n",
    "    body = request.get_data(as_text=True)\n",
    "\n",
    "    try:\n",
    "        line_handler.handle(body, signature)\n",
    "    except InvalidSignatureError:\n",
    "        abort(400)\n",
    "    return \"OK\"\n",
    "\n",
    "@app.route(\"/transcribe\", methods=[\"POST\"])\n",
    "def transcribe():\n",
    "    # 檢查請求中是否包含檔案\n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({\"error\": \"未上傳音訊檔案\"}), 400   \n",
    "    audio_file = request.files['file']\n",
    "    # 將上傳的音訊檔案保存為臨時檔案\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as temp_audio_file:\n",
    "        audio_file.save(temp_audio_file.name)   \n",
    "    # 載入音訊檔案並進行轉逐字稿\n",
    "    audio = whisper.load_audio(temp_audio_file.name)\n",
    "    result = model.transcribe(audio, language='zh')\n",
    "    # 刪除臨時檔案\n",
    "    os.remove(temp_audio_file.name)\n",
    "    # 回傳語音辨識結果\n",
    "    return jsonify({\"transcription\": result['text']})\n",
    "\n",
    "@app.route('/synthesize', methods=['POST'])\n",
    "def synthesize():\n",
    "    # 建立存放上傳文件的文件路徑\n",
    "    upload_folder = \"static/\" + datetime.now().strftime('%Y-%m-%d')\n",
    "    upload_folder = Path(upload_folder)\n",
    "    upload_folder.mkdir(exist_ok=True, parents=True)  # 如果文件路徑不存在，則新增\n",
    "    if request.method == 'POST':\n",
    "        # 取得 POST 請求中的參數\n",
    "        content = request.form.get('content', '上傳資料內容有誤')  # 要合成的文字內容\n",
    "        gender = request.form.get('gender', '1')  # 選擇的語音性別\n",
    "        tone = request.form.get('tone', '0')  # 調整的音調\n",
    "        speed = request.form.get('speed', '0')  # 調整的語速       \n",
    "        # 根據性別選擇語音\n",
    "        voices = {\n",
    "            '1': \"zh-TW-YunJheNeural\",  # 男聲參數\n",
    "            '0': \"zh-TW-HsiaoYuNeural\"  # 女聲參數\n",
    "        }\n",
    "        voice = voices.get(gender, \"zh-TW-HsiaoChenNeural\")  # 如果性別無法匹配或是沒有設定，預設使用 \"zh-TW-HsiaoChenNeural\"\n",
    "        # 設定音調和語速\n",
    "        pitch = f\"{'+' if int(tone) >= 0 else ''}{tone}Hz\"  # 音調設定，例如 +5Hz 或 -5Hz\n",
    "        rate = f\"{'+' if int(speed) >= 0 else ''}{speed}%\"  # 語速設定，例如 +10% 或 -10%\n",
    "        # 定義生成的音頻文件名稱\n",
    "        fileName = f\"{int(time.time() * 1000)}.mp3\"\n",
    "        outputFileName = str(upload_folder / fileName)  # 完整的文件路徑\n",
    "        # 構建 CLI 命令以進行語音合成\n",
    "        CLI = f'edge-tts --text \"{content}\" --write-media \"{outputFileName}\" --voice \"{voice}\" --pitch=\"{pitch}\" --rate=\"{rate}\"'\n",
    "        print(CLI)       \n",
    "        # 執行命令並處理結果\n",
    "        success, message = runcmd(CLI)\n",
    "        if success:\n",
    "            return send_file(outputFileName, mimetype='audio/mpeg')  # 成功則回傳語音島按\n",
    "        else:\n",
    "            return jsonify({\"error\": message}), 500  # 如果失敗，回傳錯誤訊息\n",
    "    return '請使用POST方法上傳數據'  # 如果不是 POST 請求，提示用戶\n",
    "\n",
    "# 定義一個路由 '/qa'，使用 POST 方法來處理問答請求\n",
    "@app.route('/qa', methods=['POST'])\n",
    "def qa():\n",
    "    # 檢查請求中是否包含 'text' 字段，如果沒有則回傳錯誤訊息\n",
    "    if 'text' not in request.json:\n",
    "        return jsonify({\"error\": \"未提供文本\"}), 400  # 400 表示客戶端錯誤\n",
    "    # 從請求中提取問題文字\n",
    "    question = request.json['text']   \n",
    "    # 呼叫 answer_question 函數來生成答案\n",
    "    answer = answer_question(question)   \n",
    "    # 將答案以 JSON 格式回傳\n",
    "    return jsonify({\"answer\": answer})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, host=\"0.0.0.0\", port=SERVER_PORT, use_reloader=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291b8a1-ef80-414d-8a5c-f94ddb324fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73387737-7193-4fd0-9925-8dc88fe538df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
